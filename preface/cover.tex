% % !Mode:: "TeX:UTF-8"

% %%  可通过增加或减少 setup/format.tex中的
% %%  第274行 \setlength{\@title@width}{8cm}中 8cm 这个参数来 控制封面中下划线的长度。

% \cheading{天津大学~2016~届本科生毕业论文}      % 设置正文的页眉，需要填上对应的毕业年份
% \ctitle{基于顾客有限理性预期的定价与供应链结构}    % 封面用论文标题，自己可手动断行
% \caffil{管理与经济学部} % 学院名称
% \csubject{工业工程}   % 专业名称
% \cgrade{2012~级}            % 年级
% \cauthor{秦昱博}            % 学生姓名
% \cnumber{3012209017}        % 学生学号
% \csupervisor{杨道箭}        % 导师姓名
% \crank{副教授}              % 导师职称

% \cdate{\the\year~年~\the\month~月~\the\day~日}

% \cabstract{
% 中文摘要一般在~400~字以内，简要介绍毕业论文的研究目的、方法、结果和结论，语言力求精炼。中英文摘要均要有关键词，一般为~3~—~7~个。字体为小四号宋体，各关键词之间要有分号。英文摘要应与中文摘要相对应，字体为小四号~Times New Roman，详见模板。
% }

% \ckeywords{关键词~1；关键词~2；关键词~3；……；关键词~7（关键词总共~3~—~7~个，最后一个关键词后面没有标点符号）}

% \eabstract{
% The upper bound of the number of Chinese characters is 400. The abstract aims at introducing the research purpose, research methods, research results, and research conclusion of graduation thesis, with refining words. Generally speaking, both the Chinese and English abstracts require the keywords, the number of which varies from 3 to 7, with a semicolon between adjacent words. The font of the English Abstract is Times New Roman, with the size of 12pt(small four).
% }

% \ekeywords{keyword 1, keyword 2, keyword 3, ……, keyword 7 (no punctuation at the end)}

% \makecover

% \clearpage


% !Mode:: "TeX:UTF-8"


\ctitle{基于生成式对抗网络的图像编辑与多模态图像转换}  %封面用论文标题，自己可手动断行
\etitle{Generative Adversarial Networks Based Image Editing and Multi Modal Image Translation}
\caffil{天津大学智能与计算学部} %学院名称
\cfirstsubjecttitle{\textbf{专业类别}}
\cfirstsubject{电子信息}   %专业
\csubjecttitle{\textbf{研究方向}}
\csubject{计算机技术}   %专业
\cauthortitle{\textbf{作者姓名}}     % 学位
\cauthor{刘家旭}   %学生姓名
\csupervisortitle{\textbf{指导教师}}
\csupervisor{朱鹏飞~~副教授} %导师姓名
\ccorsupervisortitle{\textbf{企业导师}}
\ccorsupervisor{刘昕} %导师姓名

\teachertable{
\begin{table}[h]
\centering
\song\xiaosi{
\begin{tabularx}{\textwidth}{|*{4}{>{\centering\arraybackslash}X|}}
\hline
\textbf{答辩日期}                & \multicolumn{3}{c|}{2021 年   月   日}       \\ \hline
\textbf{答辩委员会}               & \textbf{姓名} & \textbf{职称} & \textbf{工作单位} \\ \hline
\textbf{主席}                  &             &             &               \\ \hline
\multirow{2}{*}{\textbf{委员}} &             &             &               \\ \cline{2-4} 
                             &             &             &               \\ \hline
\end{tabularx}}
\end{table}}


\declaretitle{独创性声明}
\declarecontent{
本人声明所呈交的学位论文是本人在导师指导下进行的研究工作和取得的研究成果，除了文中特别加以标注和致谢之处外，论文中不包含其他人已经发表或撰写过的研究成果，也不包含为获得 {\underline{\kaiGB{\sihao{\textbf{~~天津大学~~}}}}} 或其他教育机构的学位或证书而使用过的材料。与我一同工作的同志对本研究所做的任何贡献均已在论文中作了明确的说明并表示了谢意。
}
\authorizationtitle{学位论文版权使用授权书}
\authorizationcontent{
本学位论文作者完全了解{\underline{\kaiGB{\sihao{\textbf{~~天津大学~~}}}}}有关保留、使用学位论文的规定。特授权{\underline{\kaiGB{\sihao{\textbf{~~天津大学~~}}}}} 可以将学位论文的全部或部分内容编入有关数据库进行检索，并采用影印、缩印或扫描等复制手段保存、汇编以供查阅和借阅。同意学校向国家有关部门或机构送交论文的复印件和磁盘。
}
\authorizationadd{(保密的学位论文在解密后适用本授权说明)}
\authorsigncap{学位论文作者签名:}
\supervisorsigncap{导师签名:}
\signdatecap{签字日期:}


\cdate{\CJKdigits{\the\year} 年\CJKnumber{\the\month} 月 \CJKnumber{\the\day} 日}
% 如需改成二零一二年四月二十五日的格式，可以直接输入，即如下所示
\cdate{二零二一年十月}
% \cdate{\the\year 年\the\month 月 \the\day 日} % 此日期显示格式为阿拉伯数字 如2012年4月25日
\cabstract{

生成式对抗网络（Generative Adversarial Nets，GANs）是2014年由Goodfellow等人提出的基于对抗训练的生成模型，伴随着深度学习的发展，GANs采用了更加高效的网络结构，同时研究人员不断提出更适合对抗训练的损失函数，GANs生成数据的质量有了很大提高，目前已广泛用于图像生成、图像编辑、图像转换、语音与音乐合成等领域。

虽然GANs在各类应用领域取得了极佳的成果，但仍存在一些不足，如：在图像属性编辑方面，隐空间编辑方法是当前的研究热点，但隐空间本身存在语义耦合的问题；在多模态图像转换方面，现有的方法难以对多个输入之间的相关性高效建模。本文梳理了GANs的发展和在图像方面的应用，并针对目前基于GANs的图像编辑与多模态图像转换存在的问题，做了如下的研究：

（1）为了解决GANs隐空间存在语义耦合的问题，并更准确地控制生成图像的属性，我们提出了属性一致生成对抗网络ACGAN。具体来说，我们预定义了正交的语义方向，从而形成正交的语义空间，并将隐变量分解为内容变量和属性变量，其中属性变量位于正交属性空间中，表示每种属性的强度。最后，我们在训练GANs的过程中约束属性变量与生成图像属性的一致性。由于语义方向是正交的，从而在理论上保证属性之间的解耦。

（2）多模态图像转换是图像转换的特例，可用于多模态图像数据补全。在本文中，我们探索了如何通过注意力机制对多个模态的相关性建模，并提出了一种联合注意力生成式对抗网络JAGAN。JAGAN包含两个重要的注意力模块：模态内注意力和模态间注意力。模态内注意力用于过滤与目标模态无关的信息，模态间注意力用于输入模态间的信息互补。二者的关系在于模态内注意力提供模态间注意力必需的特征兼容性。

本文在人脸、自然风景、医学图像等数据集上做了大量实验，通过定量和定量两方面的实验分析，验证了所提出模型相对于现有方法的先进性。

% 就基于GANs的图像编辑而言，研究表明，在预训练GANs的隐空间中进行特定的向量运算，我们可以人为控制生成图像的属性变化。受此影响，现有的基于GANs的图像编辑方法聚焦于在隐空间搜索属性对应的语义方向，然后将这些语义方向用于隐空间的向量运算，从而改变生成图像的属性。然而，这类方法往往存在语义耦合的问题，即在改变某一属性时，其他属性也发生了变化。为了解决语义耦合问题，并更准确地控制生成图像的属性，我们提出了属性一致生成对抗网络(Attribute Consistent Generative Adversarial Nets)，简称ACGAN。具体来说，我们预定义了正交的语义方向形成正交的语义空间，并将隐变量分解为内容变量和属性变量，其中属性变量位于正交属性空间中，表示每种属性的强度。最后，我们在训练GANs的过程中约束属性变量与生成图像属性的一致性。由于语义方向是正交的，从而在理论上保证属性之间的解耦。

% 如今从临床诊断到公共安全，许多实际场景都需要多模态图像。在多模态图像转换方面，GANs刚刚被应用到多模态图像转换，就展现出了惊人的性能，但由于多模态数据获取难度大，数据集规模往往较小，因此难以对多个输入之间的相关性高效建模，多模态图像转换仍然非常具有挑战性。在本文中，我们探索了如何通过注意力机制对多个模态的相关性建模，并提出了一种联合注意力生成式对抗网络 (Joint Attention Generative Adversarial Networks, JAGAN) 框架。JAGAN包含两个重要的注意力模块：模态内注意力和模态间注意力，充分挖掘了多模态的跨模态一致性和模态间互补性，实验表明我们提出的JAGAN生成的多模态图像质量显著高于其他方法。
    
% JAGAN通过可用的多模态图像生成缺失的模态图像。为了更有效地提取所需模态对应的多模态表示，我们使用自表示网络来驱动注意力模块。提取出的多模态特征可以与目标模态保持通道级的一致性，极大地提高了多模态图像转换性能。

}

\ckeywords{生成式对抗网络，图像编辑，多模态图像转换，语义耦合}

\eabstract{

Generative Adversarial Nets (GANs) is a generative model based on adversarial training proposed by Goodfellow et al. in 2014. With the development of deep learning, GANs adopt a more efficient network structure, and researchers continue to propose more suitable loss functions for adversarial training. The quality of the data generated by GANs is significantly improved. GANs have been widely used in image generation, image editing, image translation, voice and music synthesis, and other fields.

Although GAN has achieved inadequate results in various application fields, there are still some concerns. First, the latent space editing methods suffer from semantic entanglements.  Although GANs based image translation achieved great progress, multimodal image translation is still challenging due to the difficulty in modeling correlations between multiple inputs. Here, we discuss the development and application of GANs and do the following research to solve existing problems of GANs-based image editing and multimodal image translation.

(1) To solve the problem of semantic entanglements in latent space and control attributes of generated images more accurately, we propose Attribute Consistent Generative Adversarial Nets, termed ACGAN. We pre-defined orthogonal semantic directions, forming the orthogonal attribute space. We decompose the latent code into the content code and the attribute code. The consistency between input attribute code and attribute of generated images is optimized in the training phase. The attribute code lies in the above orthogonal space, to theoretically ensure disentanglement between attributes.

(2) Multimodal image translation is a special case of image translation, which can be used for multimodal image completion. In this article, we explored how to model the correlation of model modalities by attention mechanism, and proposed a joint attention GAN, JAGAN. JAGAN contains two important attention modules: intra-modal attention and inter-modal attention. Intra-modal attention is used to filter information that has nothing to do with the target modality, and inter-modal attention is used to complement the information between input modalities. The relationship between the two is that intra-modal attention provides the necessary feature compatibility for inter-modal attention.

In this paper, we conducted extensive experiments on datasets such as human faces, natural scenery, and medical images. Through quantitative and quantitative experimental analysis, we have verified the advanced performance of the proposed models compared to existing methods.
    

% Generative Adversarial Networks (GAN) are currently widely used in image editing, image translation and other fields.

% Recent research suggests that Generative Adversarial Networks can produce realistic images with different attributes. To precisely control each attribute of generated images, current methods search the semantic directions in the latent space of a pretrained model. Since the generative models are trained without the supervision of attribute labels, these methods yield semantic entanglement. To tackle these concerns, we propose Attribute Consistent Generative Adversarial Nets, termed ACGAN, to train an attribute-based generative model while projecting the controllable semantic directions to an orthogonal space. Specifically, we first design an attribute quantification method to obtain the continuous labels. An attribute regressor trained by them is applied to constrain the attribute consistency between the input attribute code and the generated results. We decompose the latent code into the content code and the attribute code. The attribute code lies in an orthogonal space, so as to theoretically ensure disentanglement between attributes.

% Multimodal images are required in many practical scenarios, ranging from clinical diagnosis to public security. Data missing often leads to decision bias. Although GAN based image translation achieved great progress, multimodal image translation is still challenging due to the difficulty in modeling correlations between multiple inputs. In this paper, we novelly proposed a joint attention GAN (JAGAN) framework to generate the missing modality image through available multimodal images. JAGAN contains two important attention modules: intra-modal attention and inter-modal attention, which fully explores the cross-modal consistency and inter-modal complementarity of multimodal inputs. Experiments show that quality of images generated by our proposed JAGAN is significantly higher than other methods.
% To effectively extract the multimodal representation specific to desired modality, we use a self-representation network to drive a inter-modal attention module. The extracted multimodal features can maintain kernel-level consistency with the target modality, which greatly improved the image translation performance.

}

\ekeywords{Generative Adversarial Networks, Image Editing, Multimodal Image Translation, Semantic Entanglement}

\makecover
\clearpage